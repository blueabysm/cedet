@ignore
@node Language Support Developer's Guide
@chapter Language Support Developer's Guide
@c This 'ignore' section fools texinfo-all-menus-update into creating
@c proper menus for this chapter.
@end ignore

Semantic is bundled with support for several languages such as
C, C++, Java, Python, etc.
However one of the primary gols of semantic is to provide a framework
in which anyone can add support for other languages easily.
In order to support a new lanaugage, one typically has to provide a
lexer and a parser along with appropriate semantic actions that
produce the end result of the parser - the semantic tags.

This chapter first discusses the semantic tag data structure to
familiarize the reader to the goal.  Then all the components necessary
for supporting a lanaugage is discussed starting with the writing
lexer, writing the parser, writing semantic rules, etc.
Finally several parsers bundled with semantic are discussed as case
studies.

@menu
* Tag Structure::               
* Language Support Overview::   
* Writing Lexers::              
* Writing Parsers::             
* Compiling::                   
* Debugging::                   
* Parser Error Handling::       
@end menu

@node Tag Structure
@section Tag Structure
@cindex Tag Structure

The end result of the parser for a buffer is a list of @i{tags}.
Currently each tag is a list with up to five elements:
@example
("NAME" CLASS ATTRIBUTES PROPERTIES OVERLAY)
@end example

@var{PROPERTIES} is a slot generated by the semantic parser harness,
and need not be provided by a language author.  Programmatically access
tag properties with @code{semantic-token-put} and
@code{semantic-token-get}.

@var{OVERLAY} represents positional information for this tag.  It is
automatically generated by the semantic parser harness, and need not
be provided by the language author, unless they provide a nonterminal
expansion function via @code{semantic-expand-nonterminal}.

The @var{OVERLAY} property is accessed via several functions returning
the beginning, end, and buffer of a token.  Use these functions unless
the overlay is really needed (see @ref{Tag Query}).  Depending on the
overlay in a program can be dangerous because sometimes the overlay is
replaced with an integer pair
@example
[ START END ]
@end example
when the buffer the tag belongs to is not in memory.  This happens
when a user has activated the Semantic Database @ref{semanticdb}.

To create tags for a functional or object oriented language, you can
use s series of tag creation functions.  @ref{Creating Tags}

@node Language Support Overview
@section Language Support Overview
@cindex Language Support Overview

Starting with version 2.0, @semantic{} provides many ways to add support
for a language into the @semantic{} framework.

@ignore
semantic-bovinate-toplevel is the top level function that parses the current buffer.
  semantic-parse-changes
    semantic-parse-changes-default
      semantic-edits-incremental-parser
  semantic-parse-region (overloadable)
    semantic-parse-region-default
      semantic-lex (overloadable)
        *semantic-lex-analyzer
          semantic-flex
      semantic-repeat-parse-whole-stream
        semantic-parse-stream (overloadable)
          semantic-parse-stream-default
            semantic-bovinate-stream (bovine)
          wisent-parse-stream      (wisent)
    semantic-texi-parse-region
@end ignore

@example

@ignore
semantic-post-change-major-mode-function
semantic-parser-name

semantic-toplevel-bovine-table (see semantic-active-p)
  semantic-bovinate-stream
    semantic-toplevel-bovine-table
      semantic-parse-region
        semantic-parse-region-default
          semantic-lex
          semantic-repeat-parse-whole-stream

semantic-init-db-hooks semanticdb-semantic-init-hook-fcn
semantic-init-hooks semantic-auto-parse-mode

semantic-flex-keywords-obarray (see semantic-bnf-keyword-table)
  Used by semantic-lex-keyword-symbol, semantic-lex-keyword-set,
  semantic-lex-map-keywords, semantic-flex
semantic-lex-types-obarray
@end ignore

* To support a new language, one must write a set of Emacs-Lisp
  functions that converts any valid text written in that language
  into a list of semantic tokens.  Typically this task is divided into two
  areas: a lexer and a parser.
* There are many ways of doing this.  However in almost all cases, two
* Parser converts
wisent parsers
bovine parsers
custom parsers
@end example

semantic-bovinate-toplevel calls semantic-parse-region
which returns a list of semantic tokens which get set to
semantic-toplevel-bovine-cache.

* semantic-parse-region is the first ``overloadable'' function.
  The default behavior of this is to simply call semantic-lex, then
  pass the lexical token list to semantic-repeat-parse-whole-stream.
  This in turn

@menu
* Semantic Overload Mechanism::  
@end menu

@node Semantic Overload Mechanism
@subsection Semantic Overload Mechanism

One of @semantic{}'s goals is to provide a framework for supporting a
wide range of languages.
Writing parsers for some languages are very simple, e.g.,
any dialect of Lisp family such as Emacs-Lisp and Scheme.
Parsers for many languages can be written in context free grammars
such as C, Java, Python, etc.
On the other hand, it is impossible to specify context free grammars
for other languages such as texinfo.
Yet @semantic{} already provides parsers for all these languages.

In order to support such wide range of languages,
a mechanism for customizing the parser engine at many levels
was needed to maximize the code reuse yet give each programmer
the flexibility of customizing the parser engine at many levels
of granularity.
@cindex function overloading
@cindex overloading, function
The solution that @semantic{} provides is the
@i{function overloading} mechanism which
allows one to intercept and customize the behavior
of many of the functions in the parser engine.
First the parser engine breaks down the task of parsing a language into
several steps.
Each step is represented by an Emacs-Lisp function.
Some of these are
@code{semantic-parse-region},
@code{semantic-lex},
@code{semantic-parse-stream},
@code{semantic-parse-changes},
etc.

Many built-in @semantic{} functions are declared
as being @i{over-loadable} functions, i.e., functions that do
reasonable things for most languages, but can be
customized to suit the particular needs of a given language.
All @i{over-loadable} functions then can easily be @i{over-ridden}
if necessary.
The rest of this section provides details on this @i{overloading mechanism}.

Over-loadable functions are created by defining functions
with the @code{define-overload} macro rather than the usual @code{defun}.
@code{define-overload} is a thin wrapper around @code{defun}
that sets up the function so that it can be overloaded.
An @i{over-loadable} function then can be @i{over-ridden}
in one of two ways:
@code{define-mode-overload-implementation}
and
@code{semantic-install-function-overrides}.

Let's look at a couple of examples.
@code{semantic-parse-region} is one of the top level functions
in the parser engine defined via @code{define-overload}:

@example
(define-overload semantic-parse-region
  (start end &optional nonterminal depth returnonerror)
  "Parse the area between START and END, and return any tokens found.

...
  
tokens.")
@end example

The document string was truncated in the middle above since it is not
relevant here.
The macro invocation above defines the @code{semantic-parse-region}
Emacs-Lisp function that checks first if there is an overloaded
implementation.
If one is found, then that is called.
If a mode specific implementation is not found, then the default
implementation is called which in this case is to call
@code{semantic-parse-region-default}, i.e.,
a function with the same name but with the tailing @i{-default}.

One way to overload @code{semantic-parse-region} is via
@code{semantic-install-function-overrides}.
An example from @file{semantic-texi.el} file is shown below:

@example
(defun semantic-default-texi-setup ()
  "Set up a buffer for parsing of Texinfo files."
  ;; This will use our parser.
  (semantic-install-function-overrides
   '((parse-region . semantic-texi-parse-region)
     (parse-changes . semantic-texi-parse-changes)))
  ...
  )

(add-hook 'texinfo-mode-hook 'semantic-default-texi-setup)
@end example

Above function is called whenever a buffer is setup as texinfo mode.
@code{semantic-install-function-overrides} above indicates that
@code{semantic-texi-parse-region} is to over-ride the default
implementation of @code{semantic-parse-region}.
Note the use of @code{parse-region} symbol which is
@code{semantic-parse-region} without the leading @i{semantic-} prefix.

Another way to over-ride a built-in @semantic{} function is via
@code{define-mode-overload-implementation}.
An example from @file{wisent-python.el} file is shown below.

@example
(define-mode-overload-implementation
  semantic-parse-region python-mode
  (start end &optional nonterminal depth returnonerror)
  "Over-ride in order to initialize some variables."
  (let ((wisent-python-lexer-indent-stack '(0))
        (wisent-python-explicit-line-continuation nil))
    (semantic-parse-region-default
     start end nonterminal depth returnonerror)))
@end example

Above over-rides @code{semantic-parse-region} so that for 
buffers whose major mode is @code{python-mode},
the code specified above is executed rather than the
default implementation.

@c An analogy with @xref{(elisp)Advising Functions} might be useful here.
@c Perhaps providing rational for why we don't rely on advising might
@c be good also.
@c
@c Advising is generally considered a mechanism of last resort when
@c modifying or hooking into an existing package without modifying
@c that sourde file.  Overload files advertise that they *should* be
@c overloaded, and define syntactic sugar to do so.
@c - Eric

@node Writing Lexers
@section Writing Lexers
@cindex Writing Lexers

@ignore
Are we going to support semantic-flex as well as the new lexer?

Not in the doc - Eric
@end ignore

In order to reduce a source file into a tag table, it must first be
converted into a token stream.  Tokens are syntactic elements such as
whitespace, symbols, strings, lists, and punctuation.

The lexer uses the major-mode's syntax table for conversion.
@xref{Syntax Tables,,,elisp}.
As long as that is set up correctly (along with the important
@code{comment-start} and @code{comment-start-skip} variable) the lexer
should already work for your language.

The primary entry point of the lexer is the @dfn{semantic-lex} function
shown below.
Normally, you do not need to call this function.
It is usually called by @emph{semantic-bovinate-toplevel} for you.

@anchor{semantic-lex}
@defun semantic-lex start end &optional depth length
Lexically analyze text in the current buffer between @var{START} and @var{END}.
Optional argument @var{DEPTH} indicates at what level to scan over entire
lists.  The last argument, @var{LENGTH} specifies that @dfn{semantic-lex}
should only return @var{LENGTH} tokens.  The return value is a token stream.
Each element is a list, such of the form
  (symbol start-expression .  end-expression)
where @var{SYMBOL} denotes the token type.
See @code{semantic-lex-tokens} variable for details on token types.  @var{END}
does not mark the end of the text scanned, only the end of the
beginning of text scanned.  Thus, if a string extends past @var{END}, the
end of the return token will be larger than @var{END}.  To truly restrict
scanning, use @dfn{narrow-to-region}.
@end defun

@menu
* Lexer Overview::              What is a Lexer?
* Lexer Output::                Output of a Lexical Analyzer
* Lexer Construction::          Constructing your own lexer
* Lexer Built In Analyzers::    Built in analyzers you can use
* Lexer Analyzer Construction:: Constructing your own anlyzers
* Keywords::                    Specialized lexical tokens.
* Keyword Properties::          
@end menu

@node Lexer Overview
@subsection Lexer Overview

Semantic lexer breaks up the content of an Emacs buffer into a stream of
tokens.  This process is based mostly on regular expressions which in
turn depend on the syntax table of the buffer's major mode being setup
properly.
@xref{Major Modes,,,emacs}.
@xref{Syntax Tables,,,elisp}.
@xref{Regexps,,,emacs}.

The top level lexical function @dfn{semantic-lex}, calls the function
stored in @dfn{semantic-lex-analyzer}.  The default value is the
function @dfn{semantic-flex} from version 1.4 of Semantic.

In the default lexer, the following regular expressions which rely on syntax
tables are used:

@table @code
@item @code{\\s-}
whitespace characters
@item @code{\\sw}
word constituent
@item @code{\\s_}
symbol constituent
@item @code{\\s.}
punctuation character
@item @code{\\s<}
comment starter
@item @code{\\s>}
comment ender
@item @code{\\s\\}
escape character
@item @code{\\s)}
close parenthesis character
@item @code{\\s$}
paired delimiter
@item @code{\\s\"}
string quote
@item @code{\\s\'}
expression prefix
@end table

In addition, Emacs' built-in features such as
@code{comment-start-skip},
@code{forward-comment},
@code{forward-list},
and
@code{forward-sexp}
are employed.

@node Lexer Output
@subsection Lexer Output

The lexer, @ref{semantic-lex}, scans the content of a buffer and
returns a token list.
Let's illustrate this using this simple example.

@example
00: /*
01:  * Simple program to demonstrate semantic.
02:  */
03:
04: #include <stdio.h>
05:
06: int i_1;
07:
08: int
09: main(int argc, char** argv)
10: @{
11:     printf("Hello world.\n");
12: @}
@end example

Evaluating @code{(semantic-lex (point-min) (point-max))}
within the buffer with the code above returns the following token list.
The input line and string that produced each token is shown after
each semi-colon.

@example
((punctuation     52 .  53)     ; 04: #
 (INCLUDE         53 .  60)     ; 04: include
 (punctuation     61 .  62)     ; 04: <
 (symbol          62 .  67)     ; 04: stdio
 (punctuation     67 .  68)     ; 04: .
 (symbol          68 .  69)     ; 04: h
 (punctuation     69 .  70)     ; 04: >
 (INT             72 .  75)     ; 06: int
 (symbol          76 .  79)     ; 06: i_1
 (punctuation     79 .  80)     ; 06: ;
 (INT             82 .  85)     ; 08: int
 (symbol          86 .  90)     ; 08: main
 (semantic-list   90 . 113)     ; 08: (int argc, char** argv)
 (semantic-list  114 . 147)     ; 09-12: body of main function
 )
@end example

As shown above, the token list is a list of ``tokens''.
Each token in turn is a list of the form

@example
(TOKEN-TYPE BEGINNING-POSITION . ENDING-POSITION)
@end example

@noindent
where TOKEN-TYPE is a symbol, and the other two are integers indicating
the buffer position that delimit the token such that

@lisp
(buffer-substring BEGINNING-POSITION ENDING-POSITION)
@end lisp

@noindent
would return the string form of the token.

Note that one line (line 4 above) can produce seven tokens while
the whole body of the function produces a single token.
This is because the @var{depth} parameter of @code{semantic-flex} was
not specified.
Let's see the output when @var{depth} is set to 1.
Evaluate @code{(semantic-flex (point-min) (point-max) 1)} in the same buffer.
Note the third argument of @code{1}.

@example
((punctuation    52 .  53)     ; 04: #
 (INCLUDE        53 .  60)     ; 04: include
 (punctuation    61 .  62)     ; 04: <
 (symbol         62 .  67)     ; 04: stdio
 (punctuation    67 .  68)     ; 04: .
 (symbol         68 .  69)     ; 04: h
 (punctuation    69 .  70)     ; 04: >
 (INT            72 .  75)     ; 06: int
 (symbol         76 .  79)     ; 06: i_1
 (punctuation    79 .  80)     ; 06: ;
 (INT            82 .  85)     ; 08: int
 (symbol         86 .  90)     ; 08: main

 (open-paren     90 .  91)     ; 08: (
 (INT            91 .  94)     ; 08: int
 (symbol         95 .  99)     ; 08: argc
 (punctuation    99 . 100)     ; 08: ,
 (CHAR          101 . 105)     ; 08: char
 (punctuation   105 . 106)     ; 08: *
 (punctuation   106 . 107)     ; 08: *
 (symbol        108 . 112)     ; 08: argv
 (close-paren   112 . 113)     ; 08: )

 (open-paren    114 . 115)     ; 10: @{
 (symbol        120 . 126)     ; 11: printf
 (semantic-list 126 . 144)     ; 11: ("Hello world.\n")
 (punctuation   144 . 145)     ; 11: ;
 (close-paren   146 . 147)     ; 12: @}
 )
@end example

The @var{depth} parameter ``peeled away'' one more level of ``list''
delimited by matching parenthesis or braces.
The depth parameter can be specified to be any number.
However, the parser needs to be able to handle the extra tokens.

This is an interesting benefit of the lexer having the full
resources of Emacs at its disposal.
Skipping over matched parenthesis is achieved by simply calling
the built-in functions @code{forward-list} and @code{forward-sexp}.

@node Lexer Construction
@subsection Lexer Construction

While using the default lexer is certainly an option, particularly
for grammars written in semantic 1.4 style, it is usually more
efficient to create a custom lexer for your language.

You can create a new lexer with @dfn{define-lex}.

@defun define-lex name doc &rest analyzers
Create a new lexical analyzer with @var{NAME}.
@var{DOC} is a documentation string describing this analyzer.
@var{ANALYZERS} are small code snippets of analyzers to use when
building the new @var{NAMED} analyzer.  Only use analyzers which
are written to be used in @dfn{define-lex}.
Each analyzer should be an analyzer created with @dfn{define-lex-analyzer}.
@end defun

The list of @var{analyzers}, needed here can consist of one of
several built in analyzers, or one of your own construction.  The
built in analyzers are:

@node Lexer Built In Analyzers
@subsection Lexer Built In Analyzers

@defspec semantic-lex-default-action
The default action when no other lexical actions match text.
This action will just throw an error.
@end defspec

@defspec semantic-lex-beginning-of-line
Detect and create a beginning of line token (BOL).
@end defspec

@defspec semantic-lex-newline
Detect and create newline tokens.
@end defspec

@defspec semantic-lex-newline-as-whitespace
Detect and create newline tokens.
Use this ONLY if newlines are not whitespace characters (such as when
they are comment end characters) AND when you want whitespace tokens.
@end defspec

@defspec semantic-lex-ignore-newline
Detect and create newline tokens.
Use this ONLY if newlines are not whitespace characters (such as when
they are comment end characters).
@end defspec

@defspec semantic-lex-whitespace
Detect and create whitespace tokens.
@end defspec

@defspec semantic-lex-ignore-whitespace
Detect and skip over whitespace tokens.
@end defspec

@defspec semantic-lex-number
Detect and create number tokens.
Number tokens are matched via this variable:

@defvar semantic-lex-number-expression
Regular expression for matching a number.
If this value is @code{nil}, no number extraction is done during lex.
This expression tries to match C and Java like numbers.

@example
DECIMAL_LITERAL:
    [1-9][0-9]*
  ;
HEX_LITERAL:
    0[xX][0-9a-fA-F]+
  ;
OCTAL_LITERAL:
    0[0-7]*
  ;
INTEGER_LITERAL:
    <DECIMAL_LITERAL>[lL]?
  | <HEX_LITERAL>[lL]?
  | <OCTAL_LITERAL>[lL]?
  ;
EXPONENT:
    [eE][+-]?[09]+
  ;
FLOATING_POINT_LITERAL:
    [0-9]+[.][0-9]*<EXPONENT>?[fFdD]?
  | [.][0-9]+<EXPONENT>?[fFdD]?
  | [0-9]+<EXPONENT>[fFdD]?
  | [0-9]+<EXPONENT>?[fFdD]
  ;
@end example
@end defvar

@end defspec

@defspec semantic-lex-symbol-or-keyword
Detect and create symbol and keyword tokens.
@end defspec

@defspec semantic-lex-charquote
Detect and create charquote tokens.
@end defspec

@defspec semantic-lex-punctuation
Detect and create punctuation tokens.
@end defspec

@defspec semantic-lex-punctuation-type
Detect and create a punctuation type token.
Recognized punctuations are defined in the current table of lexical
types, as the value of the `punctuation' token type.
@end defspec

@defspec semantic-lex-paren-or-list
Detect open parenthesis.
Return either a paren token or a semantic list token depending on
`semantic-lex-current-depth'.
@end defspec

@defspec semantic-lex-open-paren
Detect and create an open parenthisis token.
@end defspec

@defspec semantic-lex-close-paren
Detect and create a close paren token.
@end defspec

@defspec semantic-lex-string
Detect and create a string token.
@end defspec

@defspec semantic-lex-comments
Detect and create a comment token.
@end defspec

@defspec semantic-lex-comments-as-whitespace
Detect comments and create a whitespace token.
@end defspec

@defspec semantic-lex-ignore-comments
Detect and create a comment token.
@end defspec

@node Lexer Analyzer Construction
@subsection Lexer Analyzer Construction

Each of the previous built in analyzers are constructed using a set
of analyzer construction macros.  The root construction macro is:

@defun define-lex-analyzer name doc condition &rest forms
Create a single lexical analyzer @var{NAME} with @var{DOC}.
When an analyzer is called, the current buffer and point are
positioned in a buffer at the location to be analyzed.
@var{CONDITION} is an expression which returns @code{t} if @var{FORMS} should be run.
Within the bounds of @var{CONDITION} and @var{FORMS}, the use of backquote
can be used to evaluate expressions at compile time.
While forms are running, the following variables will be locally bound:
  @code{semantic-lex-analysis-bounds} - The bounds of the current analysis.
                  of the form (@var{START} . @var{END})
  @code{semantic-lex-maximum-depth} - The maximum depth of semantic-list
                  for the current analysis.
  @code{semantic-lex-current-depth} - The current depth of @code{semantic-list} that has
                  been decended.
  @code{semantic-lex-end-point} - End Point after match.
                   Analyzers should set this to a buffer location if their
                   match string does not represent the end of the matched text.
  @code{semantic-lex-token-stream} - The token list being collected.
                   Add new lexical tokens to this list.
Proper action in @var{FORMS} is to move the value of @code{semantic-lex-end-point} to
after the location of the analyzed entry, and to add any discovered tokens
at the beginning of @code{semantic-lex-token-stream}.
This can be done by using @dfn{semantic-lex-push-token}.
@end defun

Additionally, a simple regular expression based analyzer can be built
with:

@defun define-lex-regex-analyzer name doc regexp &rest forms
Create a lexical analyzer with @var{NAME} and @var{DOC} that will match @var{REGEXP}.
@var{FORMS} are evaluated upon a successful match.
See @dfn{define-lex-analyzer} for more about analyzers.
@end defun

@defun define-lex-simple-regex-analyzer name doc regexp toksym &optional index &rest forms
Create a lexical analyzer with @var{NAME} and @var{DOC} that match @var{REGEXP}.
@var{TOKSYM} is the symbol to use when creating a semantic lexical token.
@var{INDEX} is the index into the match that defines the bounds of the token.
Index should be a plain integer, and not specified in the macro as an
expression.
@var{FORMS} are evaluated upon a successful match @var{BEFORE} the new token is
created.  It is valid to ignore @var{FORMS}.
See @dfn{define-lex-analyzer} for more about analyzers.
@end defun

Regular expression analyzers are the simplest to create and manage.
Often, a majority of your lexer can be built this way.  The analyzer
for matching punctuation looks like this:

@example
(define-lex-simple-regex-analyzer semantic-lex-punctuation
  "Detect and create punctuation tokens."
  "\\(\\s.\\|\\s$\\|\\s'\\)" 'punctuation)
@end example

More complex analyzers for matching larger units of text to optimize
the speed of parsing and analysis is done by matching blocks.

@defun define-lex-block-analyzer name doc spec1 &rest specs
Create a lexical analyzer @var{NAME} for paired delimiters blocks.
It detects a paired delimiters block or the corresponding open or
close delimiter depending on the value of the variable
@code{semantic-lex-current-depth}.  @var{DOC} is the documentation string of the lexical
analyzer.  @var{SPEC1} and @var{SPECS} specify the token symbols and open, close
delimiters used.  Each @var{SPEC} has the form:

(@var{BLOCK-SYM} (@var{OPEN-DELIM} @var{OPEN-SYM}) (@var{CLOSE-DELIM} @var{CLOSE-SYM}))

where @var{BLOCK-SYM} is the symbol returned in a block token.  @var{OPEN-DELIM}
and @var{CLOSE-DELIM} are respectively the open and close delimiters
identifying a block.  @var{OPEN-SYM} and @var{CLOSE-SYM} are respectively the
symbols returned in open and close tokens.
@end defun

These blocks is what makes the speed of semantic's Emacs Lisp based
parsers fast.  For exmaple, by defining all text inside @{ braces @} as
a block the parser does not need to know the contents of those braces
while parsing, and can skip them all together.

@node Keywords
@subsection Keywords

Another important piece of the lexer is the keyword table (see
@ref{Writing Parsers}).  You language will want to set up a keyword table for
fast conversion of symbol strings to language terminals.

The keywords table can also be used to store additional information
about those keywords.  The following programming functions can be useful
when examining text in a language buffer.

@defun semantic-lex-keyword-p name
Return non-@code{nil} if a keyword with @var{NAME} exists in the keyword table.
Return @code{nil} otherwise.
@end defun

@defun semantic-lex-keyword-put name property value
For keyword with @var{NAME}, set its @var{PROPERTY} to @var{VALUE}.
@end defun

@defun semantic-lex-keyword-get name property
For keyword with @var{NAME}, return its @var{PROPERTY} value.
@end defun

@defun semantic-lex-map-keywords fun &optional property
Call function @var{FUN} on every semantic keyword.
If optional @var{PROPERTY} is non-@code{nil}, call @var{FUN} only on every keyword which
as a @var{PROPERTY} value.  @var{FUN} receives a semantic keyword as argument.
@end defun

@defun semantic-lex-keywords &optional property
Return a list of semantic keywords.
If optional @var{PROPERTY} is non-@code{nil}, return only keywords which have a
@var{PROPERTY} set.
@end defun

Keyword properties can be set up in a grammar file for ease of maintenance.
While examining the text in a language buffer, this can provide an easy
and quick way of storing details about text in the buffer.

@node Keyword Properties
@subsection Standard Keyword Properties

Keywords in a language can have multiple properties.  These
properties can be used to associate the string that is the keyword
with additional information.

Currently available properties are:

@table @b
@item summary
The summary property is used by semantic-summary-mode as a help
string for the keyword specified.
@end table

Notes:

Possible future properties.  This is just me musing:

@table @b
@item face
Face used for highlighting this keyword, differentiating it from the
keyword face.
@item template
@itemx skeleton
Some sort of tempo/skel template for inserting the programatic
structure associated with this keyword.
@item abbrev
As with template.
@item action
@itemx menu
Perhaps the keyword is clickable and some action would be useful.
@end table


@node Writing Parsers
@section Writing Parsers
@cindex Writing Parsers

@ignore
For the parser developer, I can think of two extra sections.  One for
semanticdb extensions,  (If a system database is needed.)  A second
for the `semantic-ctxt' extensions.  Many of the most interesting
tools will completely fail to work without local context parsing
support.

Perhaps even a section on foreign tokens.  For example, putting a
Java token into a C++ file could auto-gen a native method, just as
putting a token into a Texinfo file converts it into documentation.

In addition, in the "writing grammars" section should have
subsections as listed in the examples of the overview section.  It
might be useful to have a fourth section describing the similarities
between the two file types (by and wy) and how to use the grammar
mode.  (I'm not sure if that should be covered elsewhere.)
@end ignore

When converting a source file into a tag table it is important to
specify rules to accomplish this.  The rules are stored in the buffer
local variable @code{semantic-toplevel-bovine-table}.

While it is certainly possible to write this table yourself, it is most
likely you will want to use a compiler compiler instead @ref{Bovine Parsers}.

There are three choices for implementing your grammar.

@table @b
@item Bovine Parser
The bovine parser is the original semantic parser, and is an
implementation of an LL parser.  It is good for simple langauges.  It
has many conveniences making grammar writing easy.  The conveniences
make it less flexible than a full LALR @emph{bison} style grammar.
@item Wisent Parser
The wisent parser is a port of @emph{bison} to Emacs Lisp.  Wisent
includes the iterative error handler of the bovine parser, and has the
same error correction as traditional LALR parsers.
@item External Parser
External parsers, such as the texinfo parser can be implemented using
any means.  This allows the user of a regular expression parser for
non-regular languages, or external programs for speed.
@end table


@menu
* External Parsers::    Writing an external parser
* Bovine Parsers::      Bovine grammar based parsers
* Wisent Parsers::      Wisent grammar based parsers
* Grammer Programming Environment::    Using the grammar writing environemt
@end menu

@node External Parsers
@subsection External Parsers

The texinfo parser in @file{semantic-texi.el} is an example of an
external parser.  To make your parser work, you need to have a setup
function.

@node Bovine Parsers
@subsection Bovine Parsers

@include bovine.texi

@node Wisent Parsers
@subsection Wisent Parsers

@include wisent.texi

@node Grammer Programming Environment
@subsection Grammer Programming Environment



@c @node Grammar Files
@c @subsection Grammar Files

@ignore

This whole section appears in lang-support-guide under BNF like code,
but refers to things only in wisent.  Comenting out as there is good
stuff here.  Duping some to bovine.texi, and making it apropraite for
that mode.

-Eric


@menu
* %start::                      
* %languagemode::               
* %token::                      
@end menu

@node %start
@subsubsection %start

In Bison, one and only one nonterminal is designated as the
``start'' symbol.
In semantic, one or more nonterminals can be designated as the
``start'' symbol.

One or more start symbols can be explicitly declared following
the @b{%start}  keyword separated by spaces.

If no @b{%start} keyword is used in a grammar, then the very
first non-terminal in the grammar file is implicitly designated as the
``start'' symbol.

If one or more explicit ``start'' symbols are declared, then
the very first non-terminal is not a ``start'' symbol unless
it is explicitly declared as such.

@ignore
Submitted By: Joseph Kiniry (kiniry)
>Assigned to: Richard Y. Kim (emacsman)
Summary: Semantics of %start and %scopestart settings still unclear.

Initial Comment:
The rule specified with %start is a legal topmost
production rule, but the first rule in a BNF file is
still used as bovine-toplevel.  This is not made clear.

If a rule is specified with %scopestart then that rule
is not used/generated in the corresponding table.  This
leads to an erroneous disconnected grammar
specification.  I think that the use of this variable
needs to be reviewed and its documentation needs to be
clarified.
@c @end ignore

@defvar wisent-single-start-flag
Non-nil means allows only one start symbol like in Bison.
That is don't add extra start rules to the grammar.  This is
useful to compare the Wisent's generated automaton with the Bison's
one.
@end defvar

bovine-toplevel

@node %languagemode
@subsubsection %languagemode

@c >   A while back, I updated semantic-grammer.el to auto-run the setup
@c > function in all modes of the correct type.  It uses %languagemode to
@c > figure out what buffers to look in.  %languagemode doesn't seem to be
@c > used in wisent though.  I'm not sure if it should be or not.  Perhaps
@c > no cases call for it yet.
@c 
@c %languagemode is not used by the parser itself, but is used by
@c grammar tools, when the grammar provides it (wisent-java-tags.wy
@c for example).
@c 
@c >   Anyway, what got me more was that the setup function calls
@c > `semantic-install-function-overrides' without specifying `transient'.
@c > The effect is that I cannot get those buffers to update themselves to
@c > a new language because it throws an error.  Is there some trick I am
@c > missing?  I had to hack semantic-fw to turn off this feature while
@c > developing.
@c > 
@c >   I'm not sure what the right approach there is.  It is nice to
@c > protect some key overrides from accidental assignment later.  On the
@c > other hand, causing consternation during development is a bit
@c > annoying.  Perhaps it can quietly ignore you iff you are setting it
@c > to a value it already has.
@c 
@c I think that is a good idea.  Does the following patch works for you?
@c 
@c David
@c 
@c Index: semantic-fw.el
@c ===================================================================
@c RCS file: /cvsroot/cedet/cedet/semantic/semantic-fw.el,v
@c retrieving revision 1.16
@c diff -c -r1.16 semantic-fw.el
@c *** semantic-fw.el	15 Mar 2003 20:06:46 -0000	1.16
@c --- semantic-fw.el	26 Mar 2003 09:03:47 -0000
@c ***************
@c *** 227,232 ****
@c --- 227,235 ----
@c             ;; Binding already exists
@c             ;; Check rebind consistency
@c             (cond
@c +            ((equal (symbol-value variable) value)
@c +             ;; Just ignore rebind with the same value.
@c +             )
@c              ((get variable 'constant)
@c               (error "Can't change the value of constant `%s'"
@c                      variable))

@node %token
@subsubsection %token

%token
%left
%right

%token FOO
is a simple token without type or value

%token BAR "bar"
is a keyword

@c David said
%keyword for language keywords?

%token <symbol> symbol
@c David said:
@c I admit that "%token <symbol> symbol" can seem strange, but it is a
@c valid syntax, that defines the lexical token `symbol' for tokens of
@c type <symbol>.  The confusion here is due to
@c `semantic-lex-symbol-or-keyword', that return `symbol' tokens.

@c > 3) graphviz-dot-mode's syntax table things -, >, and other items were
@c >    of syntax type symbol, not punctuation.  Many of my %token entries
@c >    where then ignored.  I've since patched graphviz-dot-mode to use
@c >    better syntax entries.
@c 
@c That is a constraint introduced by default lexical analyzers that
@c uses Emacs syntax classes (things like "\\s<code>") in regexps.  I also
@c encountered that sort of problem when I wrote the semantic-grammar
@c lexer, and I had to concoct a `semantic-grammar-syntax-table' ;-)

@end ignore

@node Compiling
@section Compiling a language file with the bovinator

From a program you can use the function @code{semantic-bovinate-toplevel}.
This function takes one optional parameter specifying if the cache
should be refreshed.  By default, the cached results of the last parse
are always used.  Specifying that the cache should be checked will cause
it to be flushed if it is out of date.

Another function you can use is @code{semantic-bovinate-nonterminal}.
This command takes a token stream returned by the function
@code{semantic-flex} followed by a DEPTH (as above).  This takes an
additional optional argument of NONTERMINAL which is the nonterminal in
your table it is to start parsing with.

@deffn Command bovinate &optional clear
Bovinate the current buffer.  Show output in a temp buffer.
Optional argument @var{CLEAR} will clear the cache before bovinating.
@end deffn

@deffn Command semantic-clear-toplevel-cache
Clear the toplevel bovine cache for the current buffer.
Clearing the cache will force a complete reparse next time a token
stream is requested.
@end deffn

@defun semantic-bovinate-toplevel &optional checkcache
Bovinate the entire current buffer.
If the optional argument @var{CHECKCACHE} is non-@code{nil}, then flush the cache iff
there has been a size change.
@end defun

@node Debugging
@section Debugging

Writing language files using BNF is significantly easier than writing
then using regular expressions in a functional manner.  Debugging
them, however, can still prove challenging.

There are two ways to debug a language definition if it is not
behaving as expected.  One way is to debug against the source @file{.bnf}
file.  The second is to debug against the lisp table created from the
@file{.bnf} source, or perhaps written by hand.

If your language definition was written in BNF notation, debugging is
quite easy.  The command @code{bovinate-debug} will start you off.

@deffn Command bovinate-debug
Bovinate the current buffer and run in debug mode.
@end deffn

If you prefer debugging against the Lisp table, find the table in a
buffer, place the cursor in it, and use the command
@code{semantic-bovinate-debug-set-table} in it.

@deffn Command semantic-bovinate-debug-set-table &optional clear
Set the table for the next debug to be here.
Optional argument @var{CLEAR} to unset the debug table.
@end deffn

After the table is set, the @code{bovinate-debug} command can be run
at any time for the given language.

While debugging, two windows are visible.  One window shows the file
being parsed, and the syntactic token being tested is highlighted.  The
second window shows the table being used (either in the BNF source, or
the Lisp table) with the current rule highlighted.  The cursor will
sit on the specific match rule being tested against.

In the minibuffer, a brief summary of the current situation is
listed.  The first element is the syntactic token which is a list of
the form:

@example
(TYPE START . END)
@end example

The rest of the display is a list of all strings collected for the
currently tested rule.  Each time a new rule is entered, the list is
restarted.  Upon returning from a rule into a previous match list, the
previous match list is restored, with the production of the dependent
rule in the list.

Use @kbd{C-g} to stop debugging.  There are no commands for any
fancier types of debugging.

@node Parser Error Handling
@section Parser Error Handling
@cindex Parser Error Handling
